---
slug: /full-text-search
---

# 全文搜索

seekdb 通过全文索引功能提供了对文本数据的高效搜索和检索能力。

## 适用场景

在涉及大量文本数据需要进行模糊检索的场景，如果通过全表扫描来对每一行数据进行模糊查询，文本较大、数据量较多的情况下性能往往不能满足要求。另外一些复杂的查询场景，如近似匹配、相关性排序等，也难以通过改写 SQL 支撑。

全文索引可以更好的支撑这些场景，通过预先处理文本内容，建立关键词索引，全文索引可以有效提升全文搜索效率。全文索引适用于多种场景，下面列举几个具体的案例：

* 企业内部知识库：许多大型企业都会构建自己的内部知识库系统，用来存储项目文档、会议记录、研究报告等资料。使用全文索引可以帮助员工更加快速准确地找到所需信息，提高工作效率。

* 在线图书馆与电子书平台：对于提供大量书籍资源供用户阅读的服务来说，全文索引是极其重要的。用户可以输入书名、作者名字甚至是书中某段文字作为关键字来进行搜索，系统基于全文索引迅速定位到符合条件的结果。

* 新闻门户和社交媒体网站：这类平台上每天都会产生海量的新鲜内容，包括文章、帖子、评论等。利用全文索引可以让用户按照自己关心的话题、事件或是人物名称来过滤信息流，获取最相关的内容。

* 法律文书检索系统：法律行业涉及到大量的文件审阅工作，如合同、判决书、法律法规条文等。一个高效的全文搜索引擎能够极大地简化律师的工作流程，让他们能够更快地找到先例、引用条款以及相关的法律依据。

* 医疗健康信息系统：在医疗领域，医生经常需要查阅病人的历史病例、最新的医学研究论文以及其他参考资料。借助全文索引，医护人员可以更加便捷地访问相关信息，从而做出更为准确的诊断决策。

任何涉及到大量非结构化文本数据管理和查询的应用都可以考虑采用全文索引来提升检索效率。

## 全文索引介绍

在 seekdb 中，全文索引可以应用于 `CHAR`、`VARCHAR` 和 `TEXT` 类型的列。此外，seekdb 允许在主表上建立多个全文索引，并且对于同一列也可以建立多个全文索引。

非分区表和分区表上有无主键都可以创建全文索引，创建全文索引限制如下：

* 全文索引仅支持应用于 `CHAR`、`VARCHAR` 和 `TEXT` 类型的列。
* 当前版本只支持创建局部（`LOCAL`）全文索引。
* 创建全文索引时不可以指定 `UNIQUE` 关键字。
* 如果要创建涉及多列的全文索引，则必须确保这些列具有相同的字符集。

通过使用这些语法和规则，seekdb 的全文索引功能提供了对文本数据的高效搜索和检索能力。

### DML 操作

对于已创建包含全文索引的表，支持 `INSERT INTO ON DUPLICATE KEY`、`REPLACE INTO`、多表的更新/删除、以及可更新视图等复杂 DML 操作。

**示例如下：**

* INSERT INTO ON DUPLICATE KEY：

    ```sql
    INSERT INTO articles VALUES ('OceanBase', 'Fulltext search index support insert into on duplicate key')
        ON DUPLICATE KEY UPDATE title = 'OceanBase 4.3.3';
    ```

* REPLACE INTO：

    ```sql
    REPLACE INTO articles(title, context) VALUES ('Oceanbase 4.3.3', 'Fulltext search index support replace');
    ```

* 多表的更新/删除。

  1. 创建表 `tbl1`。

     ```sql
     CREATE TABLE tbl1 (a int PRIMARY KEY, b text, FULLTEXT INDEX(b));
     ```

  2. 创建表 `tbl2`。

     ```sql
     CREATE TABLE tbl2 (a int PRIMARY KEY, b text);
     ```

  3. 多个表的更新（`UPDATE`）语句。

     ```sql
     UPDATE tbl1 JOIN tbl2 ON tbl1.a = tbl2.a
         SET tbl1.b = 'dddd', tbl2.b = 'eeee';
     ```

     ```sql
     UPDATE  tbl1 JOIN tbl2 ON tbl1.a = tbl2.a SET tbl1.b = 'dddd';
     ```

     ```sql
     UPDATE tbl1 JOIN tbl2 ON tbl1.a = tbl2.a SET tbl2.b = tbl1.b;
     ```

  4. 多个表的删除（`DELETE`）语句。

     ```sql
     DELETE tbl1, tbl2 FROM tbl1 JOIN tbl2 ON tbl1.a = tbl2.a;
     ```

     ```sql
     DELETE tbl1 FROM tbl1 JOIN tbl2 ON tbl1.a = tbl2.a;
     ```

     ```sql
     DELETE tbl1 FROM tbl1 JOIN tbl2 ON tbl1.a = tbl2.a;
     ```

* 可更新视图 DML。

  1. 创建视图 `fts_view`。

     ```sql
     CREATE VIEW fts_view AS SELECT * FROM tbl1;
     ```

  2. `INSERT` 语句用于可更新视图。

     ```sql
     INSERT INTO fts_view VALUES(3, 'cccc'), (4, 'dddd');
     ```

  3. `UPDATE` 语句用于可更新视图。

     ```sql
     UPDATE fts_view SET b = 'dddd';
     ```

     ```sql
     UPDATE fts_view JOIN normal ON fts_view.a = tbl2.a
        SET fts_view.b = 'dddd', tbl2.b = 'eeee';
     ```

  4. `DELETE` 语句用于可更新视图。

     ```sql
     DELETE FROM fts_view WHERE b = 'dddd';
     ```

     ```sql
     DELETE tbl1 FROM fts_view JOIN tbl1 ON fts_view.a = tbl1.a AND 1 = 0;
     ```

### 全文索引的分词器

seekdb 的全文索引功能支持多种内置分词器，帮助用户根据业务场景选择最优的文本分词策略。默认分词器为 **Space分词器**，其他分词器需通过`WITH PARSER`参数显式指定。

**分词器列表**：

* Space 分词器
* Basic English 分词器
* IK 分词器
* Ngram 分词器
* jieba 分词器

**配置方法示例**：

在创建或修改表时，通过 `CREATE TABLE/ALTER TABLE` 语句在为表创建全文索引时，设置参数 `WITH PARSER tokenizer_option`，指定全文索引的分词器类型。更多分词器的属性参数设置参考 [创建索引](https://www.oceanbase.com/docs/common-oceanbase-database-cn-1000000002015436)。

```sql
CREATE TABLE tbl2(id INT, name VARCHAR(18), doc TEXT,
    FULLTEXT INDEX full_idx1_tbl2(name, doc)
      WITH PARSER NGRAM
      PARSER_PROPERTIES=(ngram_token_size=3));


-- 修改现有表的全文索引分词器
ALTER TABLE tbl2(id INT, name VARCHAR(18), doc TEXT,
    FULLTEXT INDEX full_idx1_tbl2(name, doc)
      WITH PARSER NGRAM
      PARSER_PROPERTIES=(ngram_token_size=3));  -- Ngram示例
```

#### Space 分词器（默认）

**概念**：

+ 以空格、标点符号（如逗号、句号）或非字母数字字符（除下划线 `_` 外）为分隔符拆分文本。
+ 分词结果仅包含长度在 `min_token_size`（默认 3）到 `max_token_size`（默认 84）之间的有效词元。
+ 中文字符被视为单个字符处理。

**适用场景**：

+ 英文等以空格分隔的语言（如 “apple watch series 9”）。
+ 中文以人工添加分隔符的场景（如 “南京 长江大桥”）。

**分词效果**：

```shell
select tokenize ("南京市长江大桥有1千米长,详见www.XXX.COM, 邮箱xx@OB.COM，一平方公里也很小 hello-word h_name", 'space');
```

```shell
+-------------------------------------------------------------------------------------------------------------+
| tokenize ("南京市长江大桥有1千米长,详见www.XXX.COM, 邮箱xx@OB.COM，一平方公里也很小 hello-word h_name", 'space')   |
+-------------------------------------------------------------------------------------------------------------+
|［"详见www", "一平方公里也很小", "xxx", "南京市长江大桥有1千米长", "邮箱xx", "word", "hello”, "h_name"］             |
+-------------------------------------------------------------------------------------------------------------+
```

**示例说明**：

+ 空格、逗号、句号等符号作为分隔符，中文连续字符视为单词。

#### Basic English（Beng) 分词器

**概念**：

* 与 Space 分词器类似，但不保留下划线 `_`，将其视为分隔符。
* 适用于英文短语分隔，但对无空格术语（如 “iPhone15”）切分效果有限。

**适用场景**：

+ 英文文档的基础检索（如日志、评论）。

**分词效果**：

```shell
select tokenize ("南京市长江大桥有1千米长,详见WWW.XXX.COM, 邮箱xx@OB.COM, 一平方公里也很小 hello-word h_name", 'beng');
```

```shell
+-----------------------------------------------------------------------------------------------------------------------+
| tokenize ("南京市长江大桥有1千米长,详见WWW.XXX.COM，邮箱xx@OB.COM, 一平方公里也很小 hello-word h_name", 'beng')                 |
+-----------------------------------------------------------------------------------------------------------------------+
|["详见www", "一平方公里也很小", "xxx", "南京市长江大桥有1千米长", "邮箱xx", "word", "hello", "name"]                              |
+-----------------------------------------------------------------------------------------------------------------------+
```

**示例说明**：

+ 下划线`_`被切分，与 Space 分词器的核心差异在于对`_`的处理。

#### Ngram 分词器

**概念**：

* 固定n值分词：默认 `n=2`，将连续非分隔符字符拆分为长度为 `n` 的子序列。
* 分隔符判定规则同 Space 分词器（保留 `_` 和数字字母）。
* 不支持长度限制参数，输出所有可能的 `n` 长度词元。

**适用场景**：

+ 短文本模糊匹配（如用户 ID、订单号）。
+ 需要固定长度特征提取的场景（如密码策略分析）。

**分词效果**：

```shell
select tokenize ("南京市长江大桥有1千米长,详见WWW.XXX.COM, 邮箱xx@OB.COM, 一平方公里也很小 hello-word h_name", 'ngram');
```

```shell
+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| tokenize ("南京市长江大桥有1千米长,详见WWW.XXX.COM, 邮箱xx@OB.COM, 一平方公里也很小 hello-word h_name", 'ngram'）                     |
+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|［"邮箱", "ww", "大桥", “ob", "me", "里也", "or", "_n", "千米", "很小", "米长", "ll", "箱x", "公里", "见w", "co", "也很", "1千", "京市", "lo", "江大", "el", "rd", "一平", "方公", "he", "am", "南京", "h_", "市长", "wo", "xx", "长江", "有1", "na", "详见", "平方", "om", "桥有"   |
+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
```

**示例说明**：

+ 默认 `n=2` 时，输出所有连续2字符的词元，包括重叠部分。

#### Ngram2 分词器

**概念**：

* 支持**动态n值范围**：通过 `min_ngram_size` 和 `max_ngram_size` 参数设置词元长度范围。
* 适用于需要多长度词元覆盖的场景。

**适用场景**： 同时需要多个固定长度词元的场景。

:::tip
用 ngram2 分词器时，需注意其内存占用较高，比如设置 `min_ngram_size` 和 `max_ngram_size` 参数范围较大时，会生成大量词元组合，可能导致资源消耗过大。
:::

**分词效果**：

```shell
select tokenize ("南京市长江大桥1千米", 'ngram2', '[{"additional_args":[{"min_ngram_size": 4},{"max_ngram_size": 6}]}]');
```

```shell
+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| tokenize("南京市长江大桥1千米", 'ngram2', '[{"additional_args":[{"min_ngram_size": 4},{"max_ngram_size": 6}]}]')                                                                                                              |
+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| ["长江大桥", "大桥1千", "江大桥1千", "市长江大桥", "京市长江", "江大桥1", "南京市长", "市长江大", "大桥1千米", "江大桥1千米", "市长江大桥1", "长江大桥1", "南京市长江", "桥1千米", "南京市长江大", "长江大桥1千", "京市长江大桥", "京市长江大"        |
+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
```

**示例说明**：

* 输出长度在4-6字符的所有连续子序列，且词元可重叠。

### IK 分词器

**概念**：

* 基于开源工具 IK Analyzer 的中文分词器，支持两种模式：

    * Smart 模式：优先输出长词，减少切分数量 (如“南京市”不切分为“南京”“市”）。
    * Max Word 模式：输出所有可能的短词（如“南京市”切分为“南京”“市”）。
* 自动识别英文单词、邮箱、URL（不含 `://`）、IP 地址等格式。

**适用场景**：中文分词

**业务场景**：

* 电商商品描述搜索（如“华为Mate60”精准匹配）。
* 社交媒体内容分析（如用户评论的关键词提取）。
* Smart 模式：会保证一个字符只会归属一个词汇，没有任何交叠，且保证组成单个词的长度尽可能长，组成的词汇尽可能少。会尝试将数词和量词组合起来，作为一个词汇输出。

    ```shell
    select tokenize("南京市长江大桥有1千米长,详见WWW.XXX.COM, 邮箱xx@OB.COM 192.168.1.1 http://www.baidu.com hello-word hello_word", 'IK', '[{"additional_args":[{"ik_mode": "smart"}]}]');
    ```

    ```shell
    +---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
    | tokenize("南京市长江大桥有1千米长,详见WWW.XXX.COM, 邮箱xx@OB.COM 192.168.1.1 http://www.baidu.com hello-word hello_word", 'IK', '[{"additional_args":[{"ik_mode": "smart"}]}]')                                                                                                      |
    +---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
    |["邮箱", "hello_word", "192.168.1.1", "hello-word", "长江大桥", "www.baidu.com", "www.xxx.com", "xx@ob.com", "长", "http", "1千米", "详见", "南京市", "有"]                                                         |
    +---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
    ```

* max_word 模式： 会把同一个字符包含到不同的分词中， 尽可能多的提供可能的词汇。

    ```shell
    select tokenize("南京市长江大桥有1千米长,详见WWW.XXX.COM, 邮箱xx@OB.COM", 'IK', '[{"additional_args":[{"ik_mode": "max_word"}]}]');
    ```

    ```shell
    +-----------------------------------------------------------------------------------------------------------------------------------------------------------------------+
    | tokenize("南京市长江大桥有1千米长,详见WWW.XXX.COM, 邮箱xx@OB.COM", 'IK', '[{"additional_args":[{"ik_mode": "max_word"}]}]')                                                |
    +-----------------------------------------------------------------------------------------------------------------------------------------------------------------------+
    |["米", "长江大桥", "市长", "干", "南京市", "南京", "千米", "xx", "www.xxx.com", "长", "www", "xx@ob.com", "长江", "ob", "XXX", "com", "详见", "l", "有", "大桥", "邮箱"]       |
    +-----------------------------------------------------------------------------------------------------------------------------------------------------------------------+
    ```

### jieba 分词器

**概念**：基于 Python 生态开源工具 `jieba` 的分词器，支持精准、全模式及搜索引擎模式。

**特点**：

* 精准模式：严格按词典切分（如“不能”不切分为“不”“能”）。
* 全模式：列出所有可能的切分组合。
* 搜索引擎模式：平衡精准与召回率（如“南京市长江大桥”→“南京”“市长”“长江大桥”）。
* 支持自定义词典和新词发现，兼容多语言（中文、英文、日文等）。

**适用场景**：

* 医疗/科技领域术语分析（如“人工智能”精准切分）。
* 多语言混合文本处理（如中英混杂的社交媒体内容）。

jieba 分词器插件需要您自行安装，编译机安装步骤，详见[分词器插件](https://www.oceanbase.com/docs/common-oceanbase-database-cn-1000000002510126)。

:::tip
当前分词器插件为实验特性，暂不建议在生产环境使用。
:::

#### 分词器选择策略

| **业务场景** | **推荐分词器** | **理由** |
| --- | --- | --- |
| 英文商品标题搜索 | **Space 或 Basic English** | 简单高效，符合英文分词习惯。 |
| 中文商品描述检索 | **IK 分词器** | 精准识别中文术语，支持自定义词典。 |
| 日志模糊匹配（如错误代码） | **Ngram 分词器** | 无需词典，覆盖无空格文本的模糊查询需求。 |
| 科技论文关键词提取 | **jieba 分词器** | 支持新词发现与复杂模式切换。 |

## 创建全文索引

可以使用 `CREATE TABLE` 语句、`CREATE FULLTEXT INDEX` 语句或 `ALTER TABLE` 语句来创建全文索引。

### 全文索引使用限制及注意事项

非分区表和分区表上有无主键都可以创建全文索引，创建全文索引限制如下：

* 全文索引仅支持应用于 `CHAR`、`VARCHAR` 和 `TEXT` 类型的列。
* 当前版本只支持创建局部（`LOCAL`）全文索引。
* 创建全文索引时不可以指定 `UNIQUE` 关键字。
* 如果要创建涉及多列的全文索引，则必须确保这些列具有相同的字符集。
* 全文索引对 Offline DDL 的支持情况请见[Offline DDL](https://www.oceanbase.com/docs/common-oceanbase-database-cn-1000000002017083)。
* 暂不支持创建列存全文索引。

### 使用 CREATE TABLE 语句创建全文索引

通过 `CREATE TABLE` 语句在创建表时即为表创建全文索引，简化语法如下：

```sql
CREATE TABLE table_name(column_name column_definition,[column_name column_definition,...]
    FULLTEXT [INDEX | KEY] [index_name](column_name)
    [WITH PARSER tokenizer_option]
    [PARSER_PROPERTIES[=](parser_properties_list)]
    [LOCAL]);

tokenizer_option:
    SPACE
    | NGRAM
    | BENG
    | IK
    | NGRAM2

parser_properties_list:
    parser_properties, [parser_properties]

parser_properties:
    min_token_size = int_value
    | max_token_size = int_value
    | ngram_token_size = int_value
    | ik_mode = 'char_value'
    | min_ngram_size = int_value
    | max_ngram_size = int_value
```

更多 `CREATE TABLE` 语法信息，参见 [CREATE TABLE](https://www.oceanbase.com/docs/common-oceanbase-database-cn-1000000002017069)。

**相关参数说明如下：**

* `table_name`：指定待创建的表的表名。

* `column_name`：指定表的列。

* `column_definition`：定义表中各列对应的数据类型。

* `FULLTEXT`：指定创建全文索引。

  :::tip
  当前版本只支持创建局部全文索引。
  :::

* `INDEX | KEY`：表示在该语句中，索引关键字使用 `INDEX` 或 `KEY` 都可以。

* `index_name`：可选，指定待创建的索引的索引名。如果未指定，默认索引名与指定的列名相同。

* `WITH PARSER tokenizer_option`：可选项，指定全文索引的分词器。取值如下：

  * `SPACE`：默认值，表示按空格进行分词。可以指定以下属性：

      |        属性      | 取值范围 |
      |------------------|---------|
      | min_token_size   | [1, 16] |
      | max_token_size   | [10, 84]|

  * `NGRAM`：表示基于 N-Gram（中文）的分词方式。可以指定以下属性：

      |        属性      | 取值范围 |
      |------------------|---------|
      | ngram_token_size | [1, 10] |

  * `NGRAM2`：表示将文本拆成 `min_ngram_size`～`max_ngram_size` 范围的连续字符。

      |        属性      | 取值范围 |
      |------------------|---------|
      | min_ngram_size   | [1, 16] |
      | max_ngram_size   | [1, 16] |

  * `BENG`：基于 Beng （基础英文）的分词方式。可以指定以下属性：

      |        属性      | 取值范围 |
      |------------------|---------|
      | min_token_size   | [1, 16] |
      | max_token_size   | [10, 84]|

  * `IK`：表示基于 IK（中文）的分词方式。当前仅支持 `utf-8` 字符集。可以指定以下属性：

      |        属性      | 取值范围 |
      |------------------|---------|
      | ik_mode          | <ul><li>`smart`</li><li>`max_word`</li></ul>|

   可以使用 [TOKENIZE](https://www.oceanbase.com/docs/common-oceanbase-database-cn-1000000002017575) 函数查看文本按照指定的分词器及 Json 形式参数的分词结果。

* `PARSER_PROPERTIES[=](parser_properties_list)`：可选项，指定分词器的属性。取值如下：

  * `min_token_size`：表示最小分词长度，默认值为 3，取值范围是 1 到 16。
  * `max_token_size`：表示最大分词长度，默认值为 84，取值范围是 10 到 84。
  * `ngram_token_size`：表示 `NGRAM` 的分词长度，只有 `NGRAM` 分词器有效，默认值为 2，取值范围是 1 到 10。
  * `ik_mode`: 表示 `IK` 分词器的分词模式。取值如下：

    * `smart`：默认值，表示词典中的词汇会被用来提高分词的准确性，词典中的词汇边界会被优先考虑，从而可能减少不必要的扩展。
    * `max_word`：表示在词典中定义的词汇会被识别出来，但不会影响分词的最大化扩展。即使词典中有定义，`max_word` 模式仍然会尝试将文本切分成更多的词汇。

* `LOCAL`：可选，指定创建局部索引。

**示例如下：**

* 创建表 `tbl1`，同时创建全文索引 `full_idx1_tbl1`。

   ```sql
   CREATE TABLE tbl1(id INT, name VARCHAR(18), date DATE, PRIMARY KEY (id), FULLTEXT INDEX full_idx1_tbl1(name));
   ```

* 创建表 `tbl2`，同时创建全文索引 `full_idx1_tbl2`，指定全文索引的分词器为 `NGRAM`，同时通过 `PARSER_PROPERTIES` 设置分词器属性。

   ```sql
   CREATE TABLE tbl2(id INT, name VARCHAR(18), doc TEXT,
       FULLTEXT INDEX full_idx1_tbl2(name, doc)
         WITH PARSER NGRAM
         PARSER_PROPERTIES=(ngram_token_size=3));
   ```

### 使用 CREATE FULLTEXT INDEX 语句创建全文索引

通过 `CREATE FULLTEXT INDEX` 语句为已有的表创建全文索引，语法如下：

```sql
CREATE FULLTEXT INDEX index_name ON table_name (column_name, [column_name ...])
    [WITH PARSER tokenizer_option]
    [PARSER_PROPERTIES[=](parser_properties_list)]
    [LOCAL];

tokenizer_option:
    SPACE
    | NGRAM
    | BENG
    | IK
    | NGRAM2

parser_properties_list:
    parser_properties, [parser_properties]

parser_properties:
    min_token_size = int_value
    | max_token_size = int_value
    | ngram_token_size = int_value
    | ik_mode = 'char_value'
    | min_ngram_size = int_value
    | max_ngram_size = int_value
```

更多 `CREATE INDEX` 语法信息，参见 [CREATE INDEX](https://www.oceanbase.com/docs/common-oceanbase-database-cn-1000000002017039)。

**相关参数说明如下：**

* `index_name`：指定待添加的索引的索引名。
* `table_name` ：指定待创建索引的表的表名。
* `column_name` ：指定对哪些列进行索引，指定多列时，各列之间用英文逗号分隔。
* `WITH PARSER tokenizer_option`：可选项，指定全文索引的分词器。取值如下：

  * `SPACE`：默认值，表示按空格进行分词。可以指定以下属性：

      |        属性      | 取值范围 |
      |------------------|---------|
      | min_token_size   | [1, 16] |
      | max_token_size   | [10, 84]|

  * `NGRAM`：表示基于 N-Gram（中文）的分词方式。可以指定以下属性：

      |        属性      | 取值范围 |
      |------------------|---------|
      | ngram_token_size | [1, 10] |

  * `NGRAM2`：表示将文本拆成 `min_ngram_size`～`max_ngram_size` 范围的连续字符。可以指定以下属性：

      |        属性      | 取值范围 |
      |------------------|---------|
      | min_ngram_size   | [1, 16] |
      | max_ngram_size   | [1, 16] |

  * `BENG`：基于 Beng （基础英文）的分词方式。可以指定以下属性：

      |        属性      | 取值范围 |
      |------------------|---------|
      | min_token_size   | [1, 16] |
      | max_token_size   | [10, 84]|

  * `IK`：表示基于 IK（中文）的分词方式。当前仅支持 `utf-8` 字符集。可以指定以下属性：

      |        属性      | 取值范围 |
      |------------------|---------|
      | ik_mode          | <ul><li>`smart`</li><li>`max_word`</li></ul>|

   可以使用 [TOKENIZE](https://www.oceanbase.com/docs/common-oceanbase-database-cn-1000000002017575) 函数查看文本按照指定的分词器及 Json 形式参数的分词结果。

* `PARSER_PROPERTIES[=](parser_properties_list)`：可选项，指定分词器的属性。取值如下：

  * `min_token_size`：表示最小分词长度，默认值为 3，取值范围是 1 到 16。
  * `max_token_size`：表示最大分词长度，默认值为 84，取值范围是 10 到 84。
  * `ngram_token_size`：表示 `NGRAM` 的分词长度，只有 `NGRAM` 分词器有效，默认值为 2，取值范围是 1 到 10。
  * `ik_mode`: 表示 `IK` 分词器的分词模式。取值如下：

    * `smart`：默认值，表示词典中的词汇会被用来提高分词的准确性，词典中的词汇边界会被优先考虑，从而可能减少不必要的扩展。
    * `max_word`：表示在词典中定义的词汇会被识别出来，但不会影响分词的最大化扩展。即使词典中有定义，`max_word` 模式仍然会尝试将文本切分成更多的词汇。

* `LOCAL`：可选项，指定创建局部索引。

**示例如下：**

创建 `tbl3` 表后，再创建全文索引 `ft_idx1_tbl3`。

1. 创建表 `tbl3`。

   ```sql
   CREATE TABLE tbl3(col1 INT, col2 VARCHAR(4096));
   ```

2. 在表 `tbl3` 上创建全文索引 `ft_idx1_tbl3`，指定全文索引的分词器为 `IK`，同时通过 `PARSER_PROPERTIES` 设置分词器属性。

   ```sql
   CREATE FULLTEXT INDEX ft_idx1_tbl3 ON tbl3(col2)
       WITH PARSER IK
       PARSER_PROPERTIES=(ik_mode='max_word');
   ```

### 使用 ALTER TABLE 语句创建全文索引

通过 `ALTER TABLE` 语句为已有的表添加全文索引，语法如下：

```sql
ALTER TABLE table_name ADD FULLTEXT [INDEX | KEY] [index_name](column_name, [column_name ...])
    [WITH PARSER tokenizer_option]
    [PARSER_PROPERTIES[=](parser_properties_list)]
    [LOCAL];

tokenizer_option:
    SPACE
    | NGRAM
    | BENG
    | IK
    | NGRAM2

parser_properties_list:
    parser_properties, [parser_properties]

parser_properties:
    min_token_size = int_value
    | max_token_size = int_value
    | ngram_token_size = int_value
    | ik_mode = 'char_value'
    | min_ngram_size = int_value
    | max_ngram_size = int_value

```

更多 `ALTER TABLE` 语法信息，参见 [ALTER TABLE](https://www.oceanbase.com/docs/common-oceanbase-database-cn-1000000002017015)。

**相关参数说明如下：**

* `table_name` ：指定待创建索引的表的表名。
* `INDEX | KEY`：可选项，表示在该语句中，索引关键字使用 `INDEX` 或 `KEY` 都可以，缺省值为 `KEY`。
* `index_name`：可选项，指定待创建的索引的索引名。如果未指定，默认索引名与指定的列名相同。
* `column_name` ：指定对哪些列进行索引，指定多列时，各列之间用英文逗号分隔。
* `WITH PARSER tokenizer_option`：可选项，指定全文索引的分词器。取值如下：

  * `SPACE`：默认值，表示按空格进行分词。可以指定以下属性：

      |        属性      | 取值范围 |
      |------------------|---------|
      | min_token_size   | [1, 16] |
      | max_token_size   | [10, 84]|

  * `NGRAM`：表示基于 N-Gram（中文）的分词方式。可以指定以下属性：

      |        属性      | 取值范围 |
      |------------------|---------|
      | ngram_token_size | [1, 10] |

  * `NGRAM2`：表示将文本拆成 `min_ngram_size`～`max_ngram_size` 范围的连续字符。可以指定以下属性：

      |        属性      | 取值范围 |
      |------------------|---------|
      | min_ngram_size   | [1, 16] |
      | max_ngram_size   | [1, 16] |

  * `BENG`：基于 Beng （基础英文）的分词方式。可以指定以下属性：

      |        属性      | 取值范围 |
      |------------------|---------|
      | min_token_size   | [1, 16] |
      | max_token_size   | [10, 84]|

  * `IK`：表示基于 IK（中文）的分词方式。当前仅支持 `utf-8` 字符集。可以指定以下属性：

      |        属性      | 取值范围 |
      |------------------|---------|
      | ik_mode          | <ul><li>`smart`</li><li>`max_word`</li></ul>|

   可以使用 [TOKENIZE](https://www.oceanbase.com/docs/common-oceanbase-database-cn-1000000002017575) 函数查看文本按照指定的分词器及 Json 形式参数的分词结果。

* `PARSER_PROPERTIES[=](parser_properties_list)`：可选项，指定分词器的属性。取值如下：

  * `min_token_size`：表示最小分词长度，默认值为 3，取值范围是 1 到 16。
  * `max_token_size`：表示最大分词长度，默认值为 84，取值范围是 10 到 84。
  * `ngram_token_size`：表示 `NGRAM` 的分词长度，只有 `NGRAM` 分词器有效，默认值为 2，取值范围是 1 到 10。
  * `ik_mode`: 表示 `IK` 分词器的分词模式。取值如下：

    * `smart`：默认值，表示词典中的词汇会被用来提高分词的准确性，词典中的词汇边界会被优先考虑，从而可能减少不必要的扩展。
    * `max_word`：表示在词典中定义的词汇会被识别出来，但不会影响分词的最大化扩展。即使词典中有定义，`max_word` 模式仍然会尝试将文本切分成更多的词汇。

* `LOCAL`：可选项，指定创建局部索引。

**示例如下：**

创建表 `tbl4` 后，然后再添加全文索引 `ft_idx1_tbl4`。

1. 创建表 `tbl4`。

   ```sql
   CREATE TABLE tbl4(col1 INT, col2 TEXT);
   ```

2. 为表 `tbl4` 添加全文索引 `ft_idx1_tbl4`，指定全文索引的分词器为 `BENG`，同时通过 `PARSER_PROPERTIES` 设置分词器属性。

   ```sql
   ALTER TABLE tbl4 ADD FULLTEXT INDEX ft_idx1_tbl4(col2)
       WITH PARSER BENG
       PARSER_PROPERTIES=(min_token_size=2, max_token_size=64);
   ```

## 使用示例

在本示例中会定义一个表以保存文档资料，并为文档设置全文索引。利用全文索引可快速匹配包含期望关键字的文档，并按相似性从高到低排序。

1. 创建表。

    ```shell
    CREATE TABLE Articles (
        id INT AUTO_INCREMENT,
        title VARCHAR(255) ,
        content TEXT ,
        PRIMARY KEY (id),
        FULLTEXT ft1 (content) WITH PARSER SPACE
        );
    ```

    ```shell
    Query OK, 0 rows affected (0.67 sec)
    ```

2. 插入数据。

    ```shell
    INSERT INTO Articles (title, content) VALUES
        ('seekdb overview', 'seekdb is an AI-native search database. It unifies relational, vector, text, JSON and GIS in a single engine, enabling hybrid search and in-database AI workflows.'),
        ('Full-Text Search in Databases', 'Full-text search allows for searching within the text of documents stored in a database. It is particularly useful for finding specific information quickly.'),
        ('Advantages of Using seekdb', 'seekdb offers several advantages such as high performance, reliability, and ease of use. ');
    ```

    ```shell
    Query OK, 3 rows affected (0.10 sec)
    Records: 3  Duplicates: 0  Warnings: 0
    ```

3. 查询表。

    ```shell
    select * from Articles;
    ```

    ```shell
    +----+-------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------+
    | id | title                         | content                                                                                                                                                            |
    +----+-------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------+
    |  1 | seekdb overview               | seekdb is an AI-native search database. It unifies relational, vector, text, JSON and GIS in a single engine, enabling hybrid search and in-database AI workflows. |
    |  2 | Full-Text Search in Databases | Full-text search allows for searching within the text of documents stored in a database. It is particularly useful for finding specific information quickly.       |
    |  3 | Advantages of Using seekdb    | seekdb offers several advantages such as high performance, reliability, and ease of use.                                                                           |
    +----+-------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------+
    3 rows in set (0.002 sec)
    ```

4. 查询匹配的文档。

    ```shell
    select id,title, content,match(content) against('seekdb database') score from Articles where match(content) against('seekdb database');
    ```

    ```shell
    +----+-------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------+
    | id | title                         | content                                                                                                                                                            | score               |
    +----+-------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------+
    |  1 | seekdb overview               | seekdb is an AI-native search database. It unifies relational, vector, text, JSON and GIS in a single engine, enabling hybrid search and in-database AI workflows. |  0.4570384669555348 |
    |  3 | Advantages of Using seekdb    | seekdb offers several advantages such as high performance, reliability, and ease of use.                                                                           |   0.240174672489083 |
    |  2 | Full-Text Search in Databases | Full-text search allows for searching within the text of documents stored in a database. It is particularly useful for finding specific information quickly.       | 0.20072992700729927 |
    +----+-------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------+
    3 rows in set (0.003 sec)
    ```

5. 利用 EXPLAIN 命令，查看查询计划并分析其性能。

    ```shell
    explain select id,title, content,match(content) against('seekdb database') score from Articles where match(content) against('seekdb database');
    ```

    ```shell
    +-------------------------------------------------------------------------------------------------------------------------------------------------+
    | Query Plan                                                                                                                                      |
    +-------------------------------------------------------------------------------------------------------------------------------------------------+
    | ==============================================================                                                                                  |
    | |ID|OPERATOR             |NAME         |EST.ROWS|EST.TIME(us)|                                                                                  |
    | --------------------------------------------------------------                                                                                  |
    | |0 |SORT                 |             |4       |139         |                                                                                  |
    | |1 |└─TEXT RETRIEVAL SCAN|articles(ft1)|4       |138         |                                                                                  |
    | ==============================================================                                                                                  |
    | Outputs & filters:                                                                                                                              |
    | -------------------------------------                                                                                                           |
    |   0 - output([articles.id], [articles.title], [articles.content], [MATCH(articles.content) AGAINST('seekdb database')]), filter(nil), rowset=16 |
    |       sort_keys([MATCH(articles.content) AGAINST('seekdb database'), DESC])                                                                     |
    |   1 - output([articles.id], [articles.content], [articles.title], [MATCH(articles.content) AGAINST('seekdb database')]), filter(nil), rowset=16 |
    |       access([articles.id], [articles.content], [articles.title]), partitions(p0)                                                               |
    |       is_index_back=true, is_global_index=false,                                                                                                |
    |       calc_relevance=true, match_expr(MATCH(articles.content) AGAINST('seekdb database')),                                                      |
    |       pushdown_match_filter(MATCH(articles.content) AGAINST('seekdb database'))                                                                 |
    +-------------------------------------------------------------------------------------------------------------------------------------------------+
    15 rows in set (0.002 sec)
    ```